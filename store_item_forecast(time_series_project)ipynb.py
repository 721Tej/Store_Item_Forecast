# -*- coding: utf-8 -*-
"""Store_Item_Forecast(Time_Series_Project)ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FypVeHVJLcMa7C06eZzNaH6oSa--6JOE

Store Item Demand - SARIMA¶
Description
This project is provided as a way to explore different time series techniques on a relatively simple and clean dataset.

We are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.

What's the best way to deal with seasonality?
Should stores be modeled separately, or can we pool them together?
Does deep learning work better than ARIMA?
Can either beat xgboost?

In this notebook, we will explore the SARIMA model and make predictions for future dates.

It is easier to remove trends and seasonality with ARIMA/SARIMA tools.
Stores can be modeled separately and pooled together.

**1. IMPORT LIBRARIES AND DATASETS**
"""

import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from pandas.plotting import autocorrelation_plot

import itertools
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX

sns.set_style('whitegrid')

import warnings
warnings.simplefilter("ignore", category=FutureWarning)
warnings.simplefilter('ignore', category=UserWarning)
warnings.simplefilter('ignore', category=ValueError)

from statsmodels.tools.sm_exceptions import ConvergenceWarning, InterpolationWarning
warnings.simplefilter('ignore', ConvergenceWarning)
warnings.simplefilter('ignore', InterpolationWarning)

sns.set_style('whitegrid')
colors = sns.color_palette('tab10')

pd.set_option('display.float_format', '{:.2f}'.format)
pd.options.display.float_format = '{:,.2f}'.format
np.set_printoptions(formatter={'float': '{: 0.2f}'.format})

train = pd.read_csv('/content/train.csv',
                    parse_dates = ['date'])

train

train.info()

# Reduce memory usage
# train[['store', 'item']] = train[['store', 'item']].astype('category')
train['store'] =  pd.to_numeric(train['store'], downcast='integer')
train['item'] =  pd.to_numeric(train['item'], downcast='integer')
train['sales'] = pd.to_numeric(train['sales'], downcast='integer')

train.info()

train.isnull().sum()

train.describe()

test = pd.read_csv('/content/test.csv',
                   parse_dates = ['date'])

test

# Reduce memory usage
# test[['store', 'item']] = test[['store', 'item']].astype('category')
test['store'] =  pd.to_numeric(test['store'], downcast='integer')
test['item'] =  pd.to_numeric(test['item'], downcast='integer')

test.info()

test.isnull().sum()

test.describe()

"""**2.DATA PREPROCESSING**

"""

# Convert 'date' to datetime format
# train['date'] = pd.to_datetime(train['date'])
# test['date'] = pd.to_datetime(test['date'])

train.set_index('date', inplace=True)

train.head()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (12, 4))

sns.histplot(train['sales'], bins=20, ax=ax1, color='#369572')
ax1.set_title('Sales Distribution')
ax1.set_xlim(train['sales'].min())
sns.boxplot(train['sales'], ax=ax2, color='#369572')

ax2.set_title('Sales Boxplot')
# ax2.set_xlim(train['sales'].min())

plt.tight_layout()
plt.show()

# Remove outliers
train = train[train['sales'] <
                    train['sales'].quantile(q = 0.98)]
train.describe()

train.info()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (16, 6))

sns.histplot(x=train['sales'], bins=20, ax=ax1, color='#369572')

ax1.set_title('Sales Distribution')
ax1.set_xlim(train['sales'].min())

sns.boxplot(train['sales'], ax=ax2, color='#369572')
ax2.set_title('Sales Boxplots')
# ax2.set_xlim(train['sales'].min())

plt.show()

"""**Smoothening the data using Differencing and Moving Average**

"""

# First-order differencing
differenced_train = train['sales'].diff()

# Rolling averages
rolling_average_train = train['sales'].rolling(window=7).mean()

plt.figure(figsize=(20, 6))

# Plot the original series
sns.lineplot(data=train, x='date', y='sales',
             color='blue', label='Original series')

# Plot the differenced series
sns.lineplot(x=differenced_train.index, y=differenced_train.values,
             color='green', label='Differenced series')

# Plot the moving average series
sns.lineplot(x=rolling_average_train.index, y=rolling_average_train.values,
             color='orange', label='Moving Average (Window=7)')

# Add the legend once after all the plots
plt.legend(title='Series')

plt.show()

"""**3. PLOT THE AUTOCORRELATION**

"""

# Autocorrelation Plot
plt.figure(figsize=(12, 6))

autocorrelation_plot(train['sales'].values[:365])

plt.title('Autocorrelation Plot')
plt.grid()
plt.show()

"""The ACF plot exhibits a sinusoidal pattern.  

This is a strong indication of seasonality in our time series data.
The repeating peaks and troughs suggest a cyclical pattern, likely related to seasonal factors.

Key observations from the plot


Cyclic pattern:
The ACF values oscillate between positive and negative, suggesting a cyclical relationship between the current value and past values.  
Decaying amplitude:
The peaks and troughs gradually become smaller, indicating that the influence of past values on the current value decreases over time.
Significant lags:
The ACF values are significant (outside the confidence bands) at lags that are multiples of a certain period (likely 12 months,
based on the context of our data).
This confirms the seasonal pattern.
Based on this ACF plot, we might consider using a Seasonal ARIMA (SARIMA) model to capture both the seasonal component and
any other trends or autoregressive/moving average components in our data.  

**4. STATIONARY CHECK**

ADF test¶

ADF test is used to determine the presence of unit root in the series, and hence helps in understand if
the series is stationary or not. The null and alternate hypothesis of this test are:

Null Hypothesis: The series has a unit root.
Alternate Hypothesis: The series has no unit root.
If the null hypothesis in failed to be rejected, this test may provide evidence that the series is non-stationary.
"""

# Set maxlag to 10 to reduce the dataset size
result = adfuller(train['sales'], maxlag=10)

print(f'ADF Statistic: {result[0]:.3f}')
print(f'p-value: {result[1]:.3f}')
print('Critical Values: ', result[4])

"""Summary¶

p-value (0.0000) is lesser than the significance level (0.05), we reject the null hypothesis.
The test suggests that the data is stationary.

KPSS test

KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for
the KPSS test are opposite that of the ADF test.

Null Hypothesis:

The process is trend stationary.
Alternate Hypothesis: The series has a unit root (series is not stationary).
"""

# KPSS Test
result = kpss(train['sales'])

print(f'KPSS Statistic: {result[0]:.3f}')
print('p-value:', result[1])
print('Critical Values:', result[3])

"""Summary¶

p-value (0.01) is lesser than the significance level (0.05), we reject the null hypothesis.
The test suggests that the data is non-stationary.

Based upon the significance level of 0.05 and the p-value:

ADF test:

The null hypothesis is rejected. Hence, the series is stationary
KPSS test: There is evidence for rejecting the null hypothesis in favour of the alternative.
Hence, the series is non-stationary as per the KPSS test.
The discrepancy between the ADF and KPSS test suggests that the time series might contain a deterministic trend.

To address this, we should:

Consider differencing the series to achieve stationarity,
Detrend the series if a trend is visually apparent,
Re-check stationarity after applying transformations.

**5. PLOT ACF & PACF**

When analyzing a time series, we should plot ACF and PACF plots before using seasonal decomposition:

ACF and PACF plots

These plots help identify the most important lags to include in a time series model, and whether there are seasonal patterns in the data.
"""

# Plot ACF and PACF Before Removing Seasonality:
fig, axes = plt.subplots(2, 1, figsize=(10, 6))

# plt.subplot(121)
plot_acf(train.sales[:1000], lags=28, ax=axes[0])

# plt.subplot(122)
plot_pacf(train.sales[:1000], lags=28, ax=axes[1])

plt.tight_layout()
plt.show()

"""Both plots clearly indicate a weekly pattern in our time series data.

The significant spikes at lags 7, 14, 21, and 28 suggest a strong correlation between the current value and values from the previous week, two weeks ago, three weeks ago, and four weeks ago.

**6. PLOT THE SEASONAL DECOMPOSITION¶**

This statistical technique breaks down a time series into its trend, seasonality, and residual components. The main goal of seasonal decomposition is to estimate seasonal effects so that we can create seasonally adjusted values.

If our data is monthly, period=12 makes sense because there are 12 months in a year.

If our data is daily, period=365 could represent annual seasonality, assuming a 365-day year.

If our data is weekly, period=53 could represent annual seasonality, assuming a 365-day year.
"""

# Decompose the time series
plt.rcParams['figure.figsize'] = (12, 9)

result = seasonal_decompose(train['sales'].values[:30],
                            model='additive', period=7)

# Plot the decomposition
result.plot()
plt.show()

"""Weekly Seasonality:

The fact that we see a clear weekly pattern in the ACF or PACF of the first 21 data points strongly suggests a weekly seasonality in our sales data. The pattern (gradually grows from Monday and peaks at Saturday, then dips on Sunday) aligns with a weekly cycle.

Data Trendy

There is an upward trend. So the first order differencing is required to remove the trend.

**7. REMOVE TREND AND SEASONALITY**
"""

# Seasonal differencing - to remove trend & seasonality
differenced_seasonal = train['sales'].diff(periods=7)

# Doubt differencing - first differencing + seasonal differencing
double_differencing = train['sales'].diff().diff(periods=7)

# Deseasonalized series
result = seasonal_decompose(train['sales'], model='additive', period=7)
seasonal = result.seasonal
deseasonalized = train['sales'] - seasonal

plt.figure(figsize=(20, 6))

# Plot the original series
sns.lineplot(data=train, x='date', y='sales',
             color='blue', label='Original series')

# Plot the seasonal differencing
sns.lineplot(x=differenced_seasonal.index, y=differenced_seasonal.values,
             color='green', label='Differenced seasonal')

# Plot the double differencing
sns.lineplot(x=double_differencing.index, y=double_differencing.values,  color='orange', label='Double differencing (Window=7)')

# Plot the deseasonalized series
sns.lineplot(x=deseasonalized.index, y=deseasonalized.values,
             color='pink', label='Deseasonalized')

# Add the legend once after all the plots
plt.legend(title='Series')

plt.show()

"""Observations
We need not only the first-order differencing but also seasonal differencing.
Thus, SARIMA or SARIMAX would be more suitable for out data.

Base model with order = (1, 1, 1) & seasonal order = (1, 1, 1, 7)

**8. SARIMA MODEL**
"""

# Function to predict future sales using SARIMAX
def predict_sarimax(store, item):

    # Filter training data for the specific store and item
    target = train[(train['store'] == store) & (train['item'] == item)]

    # Fit SARIMAX model on training data
    model = sm.tsa.statespace.SARIMAX(target['sales'], order=(1, 1, 1),
                                      seasonal_order=(1, 1, 1, 7)).fit(disp=False)

    # Use test data dates (first3 months of 2018) to predict
    future_dates = test[(test['store'] == store) & (test['item'] == item)].index

    # Make predictions for future dates
    forecast = model.get_forecast(steps=len(future_dates))
    forecast_values = forecast.predicted_mean

# Function to predict future sales using SARIMAX
def predict_sarimax(store, item):

    # Filter training data for the specific store and item
    target = train[(train['store'] == store) & (train['item'] == item)]

    # Fit SARIMAX model on training data
    model = sm.tsa.statespace.SARIMAX(target['sales'], order=(1, 1, 1),
                                      seasonal_order=(1, 1, 1, 7)).fit(disp=False)

    # Use test data dates (first3 months of 2018) to predict
    future_dates = test[(test['store'] == store) & (test['item'] == item)].index

    # Make predictions for future dates
    forecast = model.get_forecast(steps=len(future_dates))
    forecast_values = forecast.predicted_mean

    # Combine forecast with test data for future dates
    forecast_df = pd.DataFrame({'date': future_dates, 'forecast_sales': forecast_values})
    return forecast_df

# Example: Predict for store 1, item 1
results = pd.DataFrame()

print(f'Predicting for store {1} and item {1}')
sub_results = predict_sarimax(1, 1)

results = pd.concat([results, sub_results]).reset_index(drop=True)
results['date'] = test['date']

results

# Example: Predict for store 5, item 5
results = pd.DataFrame()

print(f'Predicting for store {5} and item {5}')
sub_results = predict_sarimax(5, 5)

results = pd.concat([results, sub_results]).reset_index(drop=True)
results['date'] = test['date']

results

# Example: Predict for store 10, item 10
results = pd.DataFrame()

print(f'Predicting for store {10} and item {10}')
sub_results = predict_sarimax(10, 10)

results = pd.concat([results, sub_results]).reset_index(drop=True)
results['date'] = test['date']

results

results = pd.DataFrame()

for i in range(1, 21):
  for j in range(1, 11):
    print(f'Predicting for store {j} and item {i}')
    sub_results = predict_sarimax(j, i)
    results = pd.concat([results, sub_results]).reset_index(drop=True)
    results['date'] = test['date']
    print(results)

# Save the combined DataFrame to a CSV file
final_forecasts = pd.DataFrame(results)
final_forecasts
final_forecasts.to_csv('store_item_forecasts.csv', index=False)

print('Predictions saved to store_item_forecasts.csv')

submission_df = test.loc[:, ['id']]
submission_df["sales"] = final_forecasts['forecast_sales']
submission_df

submission_df.to_csv("submission.csv", index=False)

print('Forecasts successfully submitted.')

"""**9. SARIMAX MODEL**

To incorporate exog (exogenous variables) properly into the SARIMAX model

Data Preprocessing
"""

train = pd.read_csv('/content/train.csv',
                    parse_dates = ['date'])

# Reduce memory usage
# train[['store', 'item']] = train[['store', 'item']].astype('category')
# train['sales'] = pd.to_numeric(train['sales'], downcast='integer')
train['store'] =  pd.to_numeric(train['store'], downcast='integer')
train['item'] =  pd.to_numeric(train['item'], downcast='integer')
train['sales'] = pd.to_numeric(train['sales'], downcast='integer')

train.info()

test = pd.read_csv('/content/test.csv',
                   parse_dates = ['date'])

# Reduce memory usage
# test['store'] = test['store'].astype('category')
# test['item'] = test['item'].astype('category')
test['store'] =  pd.to_numeric(test['store'], downcast='integer')
test['item'] =  pd.to_numeric(test['item'], downcast='integer')

test.info()

# Remove outliers
train = train[train['sales'] <
                    train['sales'].quantile(q = 0.98)]
train.describe()

"""Feature Engineering & Transformation"""

from datetime import date
import holidays

# Create a US holidays calendar
us_holidays = holidays.US()

# Check if each date is a holiday
train['IsHoliday'] = train['date'].apply(lambda x: x in us_holidays)
train['IsHoliday'] = train['IsHoliday'].astype(int)
train

# Check if a date is a weekend
def is_weekend(date):
    """
    Checks if a given date is a weekend.
    Args:
        date: A pandas Timestamp or datetime.date object.
    Returns:
        True if the date is a weekend, False otherwise.
    """

    # Weekday index starts from 0 (Monday),
    return date.weekday() >= 5  # so 5 and 6 represent Saturday and Sunday

# Apply the function to the 'date' column
train['is_weekend'] = train['date'].apply(is_weekend)
train['is_weekend'] = train['is_weekend'].astype(int)
train

# Check if each date is a holiday
test['IsHoliday'] = test['date'].apply(lambda x: x in us_holidays)
test['IsHoliday'] = test['IsHoliday'].astype(int)
test

# Apply the function to the 'date' column
test['is_weekend'] = test['date'].apply(is_weekend)
test['is_weekend'] = test['is_weekend'].astype(int)
test

"""** SARIMAX Model**"""

# Function to predict future sales using SARIMAX
def predict_sarimax(store, item):

    # Filter training data for the specific store and item
    target = train[(train['store'] == store) & (train['item'] == item)]

    # Fit SARIMAX model on training data
    model = sm.tsa.statespace.SARIMAX(endog=target['sales'],
                                    exog=target[['store', 'item', 'IsHoliday', 'is_weekend']],
                                    order=(1, 1, 1),
                                    seasonal_order=(1, 1, 1, 7)).fit(disp=False)

    # Use test data dates to predict (assuming test contains future data for 2018)
    future_dates = test[(test['store'] == store) & (test['item'] == item)].index
# Extract exogenous variables for future dates
    exog_future = test.loc[future_dates, ['store', 'item', 'IsHoliday', 'is_weekend']]

    # Make predictions for future dates, including exogenous variables
    forecast = model.get_forecast(steps=len(future_dates), exog=exog_future)
    forecast_values = forecast.predicted_mean

    # Optionally, get confidence intervals
    confidence_intervals = forecast.conf_int()

    # Create a DataFrame with predictions
    # forecast_df = pd.DataFrame({'date': future_dates, 'forecast_sales': forecast_values})
    forecast_df = pd.DataFrame({'date': future_dates,
                                'store': store, 'item': item,
                                'forecast_sales': forecast_values})

    forecast_df['date'] = test['date']
    forecast_df.set_index('date', inplace=True)

    return forecast_df

# Example: Predict for store 10, item 10
results = pd.DataFrame()

print(f'Predicting for store {10} and item {10}')
sub_results = predict_sarimax(10, 10)

results = pd.concat([results, sub_results]).reset_index(drop=True)
results['date'] = test['date']

results

# results = pd.DataFrame()

# for i in range(1, 51):
#   for j in range(1, 11):
#     print(f'Predicting for store {j} and item {i}')
#     sub_results = predict_sarimax(j, i)
#     results = pd.concat([results, sub_results]).reset_index(drop=True)
#     results['date'] = test['date']
#     print(results)